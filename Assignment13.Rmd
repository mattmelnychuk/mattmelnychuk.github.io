
---
output: 
  html_document:
  pdf_document: default
  word_document: default
title: "Assignment 13: Text Mining"
---

***How to do it?***: 

- Open the Rmarkdown file of this assignment ([link](fa2021_assignment13.Rmd)) in Rstudio. 

- Right under each **question**, insert  a code chunk (you can use the hotkey `Ctrl + Alt + I` to add a code chunk) and code the solution for the question. 

- `Knit` the rmarkdown file (hotkey: `Ctrl + Alt + K`) to export an html.  

-  Publish the html file to your Githiub Page. 

***Submission***: Submit the link on Github of the assignment to Canvas

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE)
```


[Sample Codes](text_mining_sample_codes.html)

-------

### Netflix Data

**1.** Download the `netflix_titles` at this [link](data/netflix_titles.csv).  Create a century variable taking two values:

```{r}
library(tidyverse)
df<- read_csv("C:/Users/student/Desktop/R 421/Math421/netflix_titles.csv")
```

    - '21' if the released_year is greater or equal to 2000, and
    
    - '20' otherwise. 
    
    ```{r}
df$century <- case_when(
  df$release_year >=2000 ~ 21,
  df$release_year <2000 ~ 20,
  TRUE ~ 20)
```
    
**2. Word Frequency**    

  a. Convert the description to tokens, remove all the stop words. What are the top 10 frequent words of movies/TV Shows in the 20th century.  Plot the bar chart of the frequency of these words. 
  
```{r}
library(tidytext)
library(knitr)

df %>% filter(century=='20') %>% 
  unnest_tokens(input = description, output = word) %>% 
  anti_join(get_stopwords()) %>% 
  count(type, word, sort = TRUE) %>% 
  head(10) %>% 
  ggplot(aes(x = n, y = reorder(word, n))) +
  geom_col() +
  labs(y = '', x = 'Frequency')
```
  
  b. What are the top 10 frequent words of movies/TV Shows in the 21st century. Plot the bar chart of the frequency of these words. Plot a side-by-side bar charts to compare the most frequent words by the two centuries. 
  
```{r}
df %>% filter(century=='21') %>% 
  unnest_tokens(input = description, output = word) %>% 
  anti_join(get_stopwords()) %>% 
  count(type, word, sort = TRUE) %>% 
  head(10) %>% 
  ggplot(aes(x = n, y = reorder(word, n))) +
  geom_col() +
  labs(y = '', x = 'Frequency')
```
  
  Side by side
```{r}
df$century <- as.factor(df$century)
df %>%
  unnest_tokens(input = description, output = word) %>% 
  anti_join(get_stopwords()) %>% 
  count(century, word, sort = TRUE) %>% 
  group_by(century) %>%
  slice_max(n, n = 10) %>% 
  ungroup() %>%
  mutate(word = reorder_within(word, by = n, within = century)) %>%
  ggplot(aes(n, word, fill = century)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~century, scales = "free") +
  labs(x = "Frequency",
       y = NULL)+
  scale_y_reordered() 
```
  

**3. Word Cloud**

  a. Plot the word cloud of the words in the descriptions in the movies/TV Shows in the 20th century.
```{r, warning=FALSE}
library(wordcloud) 
pal <- brewer.pal(8,"Dark2")

df %>% filter(century=='20') %>% 
  unnest_tokens(input = description, output = word) %>% 
  anti_join(get_stopwords()) %>% 
  count(word, sort = TRUE) %>%
  with(wordcloud(word, n, random.order = FALSE, max.words = 50, colors=pal))
```
  
  
  b. Plot the word cloud of the words in the descriptions in the movies/TV Shows in the 21st century. 
```{r, warning=FALSE}
df %>% filter(century=='21') %>% 
  unnest_tokens(input = description, output = word) %>% 
  anti_join(get_stopwords()) %>% 
  count(word, sort = TRUE) %>%
  with(wordcloud(word, n, random.order = FALSE, max.words = 50, colors=pal))
```
  

**4. Sentiment Analysis**

  a. Is movies/TV Shows in the 21st century tends to be more positive than those in 20th century?  Use the sentiment analysis by `Bing` lexicons to address the question. 
```{r}
df %>%
    mutate(century = if_else(release_year>=2000, '21','20')) %>% 
    unnest_tokens(input = description, output = word) %>% 
    anti_join(get_stopwords()) %>% 
    count(century, word, sort = TRUE) %>%
    group_by(century) %>% 
    inner_join(get_sentiments("bing")) %>%
    filter(!is.na(sentiment)) %>%
    count(sentiment, sort = TRUE) %>% 
    group_by(century) %>% 
    mutate(n = n/sum(n)) %>% 
    ggplot(aes(century, n, fill=sentiment))+geom_col(position = 'fill')+
    labs(y='Relative Frequency', x ='')
```
  21st century movies and TV shows are slightly less positive than 20th century productions.
  
  b. Do sentiment analysis using `nrc` and `afinn` lexicons.  Give your comments on the results.
```{r}
df %>%
    unnest_tokens(input = description, output = word) %>% 
    anti_join(get_stopwords()) %>% 
    count(century, word, sort = TRUE) %>%
    group_by(century) %>% 
    inner_join(get_sentiments("nrc")) %>%
    filter(!is.na(sentiment)) %>%
    count(sentiment, sort = TRUE) %>% 
    group_by(century) %>% 
    mutate(n = n/sum(n)) %>% 
    ggplot(aes(sentiment, n, fill=century))+geom_col(position = 'fill')+
    labs(y='Relative Frequency', x ='')
```
  Relatively speaking, negative and disgust are sentiments that show up more frequently in the 21st century than in the 20th century. Trust shows up more frequently in the 20th century, and other than that most of the relative frequencies of each tend to be split about 50/50.
  
```{r, warning=FALSE}
library(tidyverse)
df %>%
    unnest_tokens(input = description, output = word) %>% 
    anti_join(get_stopwords()) %>% 
    count(century, word, sort = TRUE) %>%
    group_by(century) %>% 
    inner_join(get_sentiments("afinn")) %>%
    mutate(sentiment = value) %>% 
    filter(!is.na(sentiment)) %>%
    count(sentiment, sort = TRUE) %>% 
    group_by(century) %>% 
    mutate(n = n/sum(n)) %>% 
    ggplot(aes(century, n, fill=factor(sentiment)))+geom_col(position = 'dodge')+
    labs(y='Relative Frequency', fill = 'Sentiment', x = '')
```
  In general, the 21st century may be a bit more negative because of a slightly higher concentration of -2 valued words, but for the most part the two graphs look very similar.

**5. Modeling**

  a. Use the description to predict if a movie/TV show is in 20th or 21st century. Give the accuracy and plot the confusion matrix table. 
  
  I had to change the max tokens from 500 down to 25 and p from 0.7 to 0.3 to help with processing speed.
  
```{r}
library(caret)
library(themis)
library(textrecipes)

# Select data and set target 
df2 <- df %>% 
  mutate(target = century) %>% 
  select(target, description) 

df2$target <- as.factor(df2$target)
# Convert text data to numeric variables
a <- recipe(target~description,
       data = df2) %>% 
  step_tokenize(description) %>% 
  step_tokenfilter(description, max_tokens = 25) %>% 
  step_tfidf(description) %>% 
  step_normalize(all_numeric_predictors()) %>% 
  step_smote(target) %>% 
  prep()
df2 <- juice(a)

# Using Caret for modeling
set.seed(2020)
splitIndex <- createDataPartition(df2$target, p = .3, 
                                  list = FALSE)
df_train <- df2[ splitIndex,]
df_test <- df2[-splitIndex,]

forest_ranger <- train(target~., data=df_train, 
                        method = "ranger")

pred <- predict(forest_ranger, df_test)

cm <- confusionMatrix(data = pred, reference = df_test$target)
cm$overall[1]
```
  
```{r}
d = data.frame(pred = pred, obs = df_test$target)
library(yardstick)
d %>% conf_mat(pred, obs) %>% autoplot
```
  
  
  
  b. Create variable century2 taking three following values. (Hint: You can use the case_when function to do this)

    - `21` if released_year is greater or equal to 2000
    
    - `second_half_20`if released_year is greater than or equal to 1950 and less than 2000
    
    - `first_half_20` otherwise
    
```{r}
df$century2 <- case_when(
  df$release_year >=2000 ~ '21',
  df$release_year <1950 ~ 'first_half_20',
  TRUE ~ 'second_half_20')
```
    
    
  Predict century2 using the descriptions. Give the accuracy and plot the confusion matrix table. (Notice that the codes for 8 should still work for this question)
  
```{r}
library(caret)
library(themis)
library(textrecipes)

# Select data and set target 
df2 <- df %>% 
  mutate(target = century2) %>% 
  select(target, description) 

df2$target <- as.factor(df2$target)
# Convert text data to numeric variables
a <- recipe(target~description,
       data = df2) %>% 
  step_tokenize(description) %>% 
  step_tokenfilter(description, max_tokens = 25) %>% 
  step_tfidf(description) %>% 
  step_normalize(all_numeric_predictors()) %>% 
  step_smote(target) %>% 
  prep()
df2 <- juice(a)

# Using Caret for modeling
set.seed(2020)
splitIndex <- createDataPartition(df2$target, p = .3, 
                                  list = FALSE)
df_train <- df2[ splitIndex,]
df_test <- df2[-splitIndex,]

forest_ranger <- train(target~., data=df_train, 
                        method = "ranger")

pred <- predict(forest_ranger, df_test)

cm <- confusionMatrix(data = pred, reference = df_test$target)
cm$overall[1]
```
  
```{r}
d = data.frame(pred = pred, obs = df_test$target)
library(yardstick)
d %>% conf_mat(pred, obs) %>% autoplot
```
  

**6.** Create another categorical variable from the data and do the following

```{r}
table(df$rating)
df$RMA <- case_when(
  df$rating == 'R' ~ 'Mature/R',
  df$rating == 'UR' ~ 'Mature/R',
  df$rating == 'NR' ~ 'Mature/R',
  df$rating == 'TV-MA'~ 'Mature/R',
  TRUE ~ 'Safe')

```


    - Plot side-by-side word frequency by different categories of the newly created variable
    
```{r}
df %>%
  unnest_tokens(input = description, output = word) %>% 
  anti_join(get_stopwords()) %>% 
  count(RMA, word, sort = TRUE) %>% 
  group_by(RMA) %>%
  slice_max(n, n = 10) %>% 
  ungroup() %>%
  mutate(word = reorder_within(word, by = n, within = RMA)) %>%
  ggplot(aes(n, word, fill = RMA)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~RMA, scales = "free") +
  labs(x = "Frequency",
       y = NULL)+
  scale_y_reordered()
```
    
    
    - Plot word clouds on different categories of the newly created variable
    
```{r, warning = FALSE}
library(wordcloud) 
pal <- brewer.pal(8,"Dark2")

df %>% filter(RMA=='Safe') %>% 
  unnest_tokens(input = description, output = word) %>% 
  anti_join(get_stopwords()) %>% 
  count(word, sort = TRUE) %>%
  with(wordcloud(word, n, random.order = FALSE, max.words = 50, colors=pal))
```
    
    
```{r, warning = FALSE}
df %>% filter(RMA=='Mature/R') %>% 
  unnest_tokens(input = description, output = word) %>% 
  anti_join(get_stopwords()) %>% 
  count(word, sort = TRUE) %>%
  with(wordcloud(word, n, random.order = FALSE, max.words = 50, colors=pal))
```
    
    
    - Do sentiment analysis to compare different categories of the newly created variable
    
```{r}
df %>%
    unnest_tokens(input = description, output = word) %>% 
    anti_join(get_stopwords()) %>% 
    count(RMA, word, sort = TRUE) %>%
    group_by(RMA) %>% 
    inner_join(get_sentiments("nrc")) %>%
    filter(!is.na(sentiment)) %>%
    count(sentiment, sort = TRUE) %>% 
    group_by(RMA) %>% 
    mutate(n = n/sum(n)) %>% 
    ggplot(aes(sentiment, n, fill=RMA))+geom_col(position = 'fill')+
    labs(y='Relative Frequency', x ='')
```
    Surprisingly, there does not seem to be much of a difference in sentiments between shows that are rated R/MA or safer productions.
    
    - Predict the newly created variable using the description. Give the accuracy and plot the confusion matrix table. 
    
```{r}
library(caret)
library(themis)
library(textrecipes)

# Select data and set target 
df2 <- df %>% 
  mutate(target = RMA) %>% 
  select(target, description) 

df2$target <- as.factor(df2$target)
# Convert text data to numeric variables
a <- recipe(target~description,
       data = df2) %>% 
  step_tokenize(description) %>% 
  step_tokenfilter(description, max_tokens = 25) %>% 
  step_tfidf(description) %>% 
  step_normalize(all_numeric_predictors()) %>% 
  step_smote(target) %>% 
  prep()
df2 <- juice(a)

# Using Caret for modeling
set.seed(2020)
splitIndex <- createDataPartition(df2$target, p = .3, 
                                  list = FALSE)
df_train <- df2[ splitIndex,]
df_test <- df2[-splitIndex,]

forest_ranger <- train(target~., data=df_train, 
                        method = "ranger")

pred <- predict(forest_ranger, df_test)

cm <- confusionMatrix(data = pred, reference = df_test$target)
cm$overall[1]
```

This model is not very accurate; it is almost equivalent to just guessing 50/50.

```{r}
d = data.frame(pred = pred, obs = df_test$target)
library(yardstick)
d %>% conf_mat(pred, obs) %>% autoplot
```
    
-------

### Animal Reviews Data

**7.**  Download the animal reviews data at this [link](data/user_reviews.tsv).  Read the data using `read_tsv()` function.
```{r}
library(tidyverse)
df3 <- read_tsv("C:/Users/student/Desktop/R 421/user_reviews.tsv")
```


**8.** Create a `rating` variable taking value `good` if the grade is greater than 7 and `bad` otherwise. 

```{r}
df3$rating <- case_when(
  df3$grade > 7 ~ 'good',
  TRUE ~ 'bad')
```


**9.** Do the follows. Notice that the text information is in the `text` variable. 

    - Plot side-by-side word frequency by different categories of the `rating` variable
    
```{r}
df3 %>%
  unnest_tokens(input = text, output = word) %>% 
  anti_join(get_stopwords()) %>% 
  count(rating, word, sort = TRUE) %>% 
  group_by(rating) %>%
  slice_max(n, n = 10) %>% 
  ungroup() %>%
  mutate(word = reorder_within(word, by = n, within = rating)) %>%
  ggplot(aes(n, word, fill = rating)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~rating, scales = "free") +
  labs(x = "Frequency",
       y = NULL)+
  scale_y_reordered() 
```
    
    - Plot word clouds on different categories of the `rating` variable
    
```{r}
library(wordcloud) 
pal <- brewer.pal(8,"Dark2")

df3 %>% filter(rating=='good') %>% 
  unnest_tokens(input = text, output = word) %>% 
  anti_join(get_stopwords()) %>% 
  count(word, sort = TRUE) %>%
  with(wordcloud(word, n, random.order = FALSE, max.words = 50, colors=pal))
```

```{r}
library(wordcloud) 
pal <- brewer.pal(8,"Dark2")

df3 %>% filter(rating=='bad') %>% 
  unnest_tokens(input = text, output = word) %>% 
  anti_join(get_stopwords()) %>% 
  count(word, sort = TRUE) %>%
  with(wordcloud(word, n, random.order = FALSE, max.words = 50, colors=pal))
```

    - Do sentiment analysis to compare different categories of the `rating` variable
    
```{r, warning = FALSE}
library(tidyverse)
df3 %>%
    unnest_tokens(input = text, output = word) %>% 
    anti_join(get_stopwords()) %>% 
    count(rating, word, sort = TRUE) %>%
    group_by(rating) %>% 
    inner_join(get_sentiments("afinn")) %>%
    mutate(sentiment = value) %>% 
    filter(!is.na(sentiment)) %>%
    count(sentiment, sort = TRUE) %>% 
    group_by(rating) %>% 
    mutate(n = n/sum(n)) %>% 
    ggplot(aes(rating, n, fill=factor(sentiment)))+geom_col(position = 'dodge')+
    labs(y='Relative Frequency', fill = 'Sentiment', x = '')
```
    This chart makes sense because bad reviews have negative sentiment more often and good reviews have more positive reviews.
    
    - Predict the rating using the reviews (`text` variable). Give the accuracy and plot the confusion matrix table.
    
```{r}
library(caret)
library(themis)
library(textrecipes)

# Select data and set target 
df4 <- df3 %>% 
  mutate(target = rating) %>% 
  select(target, text) 

df4$target <- as.factor(df4$target)
# Convert text data to numeric variables
a <- recipe(target~text,
       data = df4) %>% 
  step_tokenize(text) %>% 
  step_tokenfilter(text, max_tokens = 25) %>% 
  step_tfidf(text) %>% 
  step_normalize(all_numeric_predictors()) %>% 
  step_smote(target) %>% 
  prep()
df4 <- juice(a)

# Using Caret for modeling
set.seed(2020)
splitIndex <- createDataPartition(df4$target, p = .3, 
                                  list = FALSE)
df_train <- df4[ splitIndex,]
df_test <- df4[-splitIndex,]

forest_ranger <- train(target~., data=df_train, 
                        method = "ranger")

pred <- predict(forest_ranger, df_test)

cm <- confusionMatrix(data = pred, reference = df_test$target)
cm$overall[1]

```

```{r}
d = data.frame(pred = pred, obs = df_test$target)
library(yardstick)
d %>% conf_mat(pred, obs) %>% autoplot
```

